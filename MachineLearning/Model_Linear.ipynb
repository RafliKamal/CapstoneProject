{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import library yang diperlukan\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4lsDXLKFV_SI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat dataset dari CSV\n",
        "url = '/content/data_keuangan.csv'\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "51jQfgbkWGPX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memisahkan fitur (X) dan target (y)\n",
        "X = df.drop(columns=['Laba Bersih'])  # Semua kolom kecuali Laba Bersih\n",
        "y = df['Laba Bersih']  # Target adalah Laba Bersih\n",
        "\n",
        "# Membagi dataset menjadi data latih dan data uji\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Menstandarisasi data (opsional, tetapi sering membantu dengan regresi)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "GBnCBFHKWeq1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "my8szCXnAYBP"
      },
      "outputs": [],
      "source": [
        "# Menyusun model Linear Regression dengan TensorFlow\n",
        "model = tf.keras.Sequential([\n",
        "   tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
        "   tf.keras.layers.Dense(1024, activation='relu'),\n",
        "   tf.keras.layers.Dense(512, activation='relu'),\n",
        "   tf.keras.layers.Dense(256, activation='relu'),\n",
        "   tf.keras.layers.Dense(128, activation='relu'),\n",
        "   tf.keras.layers.Dense(64, activation='relu'),\n",
        "   tf.keras.layers.Dense(32, activation='relu'),\n",
        "   tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyusun model dengan optimizer dan loss function\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Melatih model\n",
        "model.fit(X_train_scaled, y_train, epochs=250, batch_size=32, validation_data=(X_test_scaled, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEuV5AyRXfbU",
        "outputId": "dd424281-8027-4749-a09c-ecb67d8ca3b2"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1935761.7500 - val_loss: 92771.6797\n",
            "Epoch 2/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 194088.0781 - val_loss: 83439.2812\n",
            "Epoch 3/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 47512.7305 - val_loss: 11244.3076\n",
            "Epoch 4/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2909.0110 - val_loss: 9025.4727\n",
            "Epoch 5/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 560.7117 - val_loss: 7323.2529\n",
            "Epoch 6/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 141.7300 - val_loss: 7369.7124\n",
            "Epoch 7/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 106.6756 - val_loss: 7499.7676\n",
            "Epoch 8/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 70.4594 - val_loss: 7213.2969\n",
            "Epoch 9/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 83.6509 - val_loss: 7375.7749\n",
            "Epoch 10/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 93.9381 - val_loss: 7080.3511\n",
            "Epoch 11/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 73.3947 - val_loss: 7371.2417\n",
            "Epoch 12/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 65.7689 - val_loss: 7288.4761\n",
            "Epoch 13/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 76.9065 - val_loss: 7332.4189\n",
            "Epoch 14/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 101.0273 - val_loss: 7035.5298\n",
            "Epoch 15/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 93.2988 - val_loss: 7647.7285\n",
            "Epoch 16/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 84.1776 - val_loss: 7091.8711\n",
            "Epoch 17/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 80.6952 - val_loss: 7319.4121\n",
            "Epoch 18/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 69.7699 - val_loss: 7125.2476\n",
            "Epoch 19/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 131.6732 - val_loss: 7101.6812\n",
            "Epoch 20/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 99.5991 - val_loss: 7362.6777\n",
            "Epoch 21/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 87.0247 - val_loss: 7204.2598\n",
            "Epoch 22/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 89.8463 - val_loss: 7269.4336\n",
            "Epoch 23/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 150.2181 - val_loss: 7238.9292\n",
            "Epoch 24/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 235.0067 - val_loss: 7167.4463\n",
            "Epoch 25/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 164.3092 - val_loss: 7701.4712\n",
            "Epoch 26/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 174.8232 - val_loss: 7239.0820\n",
            "Epoch 27/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 132.8589 - val_loss: 7466.7515\n",
            "Epoch 28/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 322.1249 - val_loss: 8788.5508\n",
            "Epoch 29/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1030.7212 - val_loss: 8189.5737\n",
            "Epoch 30/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 837.2438 - val_loss: 6888.4814\n",
            "Epoch 31/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 455.7231 - val_loss: 8692.5000\n",
            "Epoch 32/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3012.4653 - val_loss: 18529.0273\n",
            "Epoch 33/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 10271.9678 - val_loss: 22135.0879\n",
            "Epoch 34/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 15006.2061 - val_loss: 41792.2070\n",
            "Epoch 35/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 57099.1523 - val_loss: 53302.1133\n",
            "Epoch 36/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 48769.2344 - val_loss: 182391.4531\n",
            "Epoch 37/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 117077.3750 - val_loss: 224488.3438\n",
            "Epoch 38/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 263621.8750 - val_loss: 60235.1641\n",
            "Epoch 39/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 44800.8789 - val_loss: 26262.4980\n",
            "Epoch 40/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 24585.7520 - val_loss: 27954.5059\n",
            "Epoch 41/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25560.0078 - val_loss: 19623.3906\n",
            "Epoch 42/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15029.9668 - val_loss: 19536.5332\n",
            "Epoch 43/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18310.4512 - val_loss: 43153.3164\n",
            "Epoch 44/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18557.5371 - val_loss: 21474.6406\n",
            "Epoch 45/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11266.5703 - val_loss: 12298.3975\n",
            "Epoch 46/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5950.6704 - val_loss: 13559.5498\n",
            "Epoch 47/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5523.0532 - val_loss: 22444.2480\n",
            "Epoch 48/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 20785.4258 - val_loss: 28978.2246\n",
            "Epoch 49/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 20420.6602 - val_loss: 70078.4297\n",
            "Epoch 50/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 43649.8711 - val_loss: 43059.9609\n",
            "Epoch 51/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 31874.4492 - val_loss: 38653.8945\n",
            "Epoch 52/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 40931.5117 - val_loss: 36202.8125\n",
            "Epoch 53/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 16274.4316 - val_loss: 12477.4111\n",
            "Epoch 54/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8481.3633 - val_loss: 41621.4688\n",
            "Epoch 55/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 37510.6367 - val_loss: 41410.4961\n",
            "Epoch 56/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18581.0820 - val_loss: 40521.5312\n",
            "Epoch 57/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 43852.8633 - val_loss: 44384.6797\n",
            "Epoch 58/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 40383.2539 - val_loss: 26134.5781\n",
            "Epoch 59/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14568.7754 - val_loss: 21803.8047\n",
            "Epoch 60/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14536.6006 - val_loss: 22428.5645\n",
            "Epoch 61/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 15110.4365 - val_loss: 23392.3418\n",
            "Epoch 62/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 36156.7891 - val_loss: 42286.1484\n",
            "Epoch 63/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 46839.3125 - val_loss: 21879.8672\n",
            "Epoch 64/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15997.4805 - val_loss: 14165.2178\n",
            "Epoch 65/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 15381.9941 - val_loss: 65063.7812\n",
            "Epoch 66/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 63678.2148 - val_loss: 321618.9688\n",
            "Epoch 67/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 142479.7188 - val_loss: 58253.5742\n",
            "Epoch 68/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 57916.4766 - val_loss: 40928.6602\n",
            "Epoch 69/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 68340.9922 - val_loss: 63059.7109\n",
            "Epoch 70/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 52809.4844 - val_loss: 19468.8633\n",
            "Epoch 71/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 27365.3145 - val_loss: 14194.1816\n",
            "Epoch 72/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 13369.7344 - val_loss: 29105.9902\n",
            "Epoch 73/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 23031.3613 - val_loss: 28110.1895\n",
            "Epoch 74/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 34424.9922 - val_loss: 207584.5938\n",
            "Epoch 75/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 162008.5938 - val_loss: 217212.8438\n",
            "Epoch 76/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 734575.0625 - val_loss: 371048.9688\n",
            "Epoch 77/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 675564.2500 - val_loss: 313681.0625\n",
            "Epoch 78/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 605630.5625 - val_loss: 1150242.5000\n",
            "Epoch 79/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 814204.5625 - val_loss: 930783.4375\n",
            "Epoch 80/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 426008.0000 - val_loss: 199961.2812\n",
            "Epoch 81/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 202464.4219 - val_loss: 46184.8359\n",
            "Epoch 82/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 32352.0840 - val_loss: 15228.8799\n",
            "Epoch 83/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14037.2002 - val_loss: 12955.1162\n",
            "Epoch 84/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4797.0513 - val_loss: 9418.3027\n",
            "Epoch 85/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2115.6245 - val_loss: 10284.0410\n",
            "Epoch 86/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1673.6519 - val_loss: 8811.5166\n",
            "Epoch 87/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 916.4325 - val_loss: 8619.5098\n",
            "Epoch 88/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 305.1772 - val_loss: 7703.6294\n",
            "Epoch 89/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 125.1106 - val_loss: 7626.4473\n",
            "Epoch 90/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 123.4117 - val_loss: 7731.2012\n",
            "Epoch 91/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 170.5331 - val_loss: 7348.3613\n",
            "Epoch 92/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 104.4504 - val_loss: 7254.4775\n",
            "Epoch 93/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 114.3212 - val_loss: 7661.3320\n",
            "Epoch 94/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 112.2780 - val_loss: 7205.9502\n",
            "Epoch 95/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 65.2860 - val_loss: 7479.7656\n",
            "Epoch 96/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 117.2009 - val_loss: 7304.0015\n",
            "Epoch 97/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 108.9628 - val_loss: 7382.2070\n",
            "Epoch 98/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 111.2077 - val_loss: 7718.3013\n",
            "Epoch 99/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 146.7382 - val_loss: 7336.4434\n",
            "Epoch 100/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 127.4623 - val_loss: 6892.3164\n",
            "Epoch 101/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 106.8862 - val_loss: 7445.2188\n",
            "Epoch 102/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 135.3878 - val_loss: 7235.3550\n",
            "Epoch 103/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 76.1612 - val_loss: 7525.8862\n",
            "Epoch 104/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 131.9453 - val_loss: 7028.5767\n",
            "Epoch 105/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 303.1187 - val_loss: 7580.0688\n",
            "Epoch 106/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 142.1131 - val_loss: 7387.9663\n",
            "Epoch 107/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 91.1750 - val_loss: 7384.9863\n",
            "Epoch 108/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 72.2666 - val_loss: 7037.1177\n",
            "Epoch 109/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 75.6137 - val_loss: 7303.0527\n",
            "Epoch 110/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 56.3515 - val_loss: 6974.7300\n",
            "Epoch 111/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 113.0015 - val_loss: 7278.6587\n",
            "Epoch 112/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 171.3664 - val_loss: 7226.7773\n",
            "Epoch 113/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 177.0016 - val_loss: 7089.1255\n",
            "Epoch 114/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 231.8624 - val_loss: 7144.0151\n",
            "Epoch 115/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 113.1416 - val_loss: 7049.1089\n",
            "Epoch 116/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 124.7718 - val_loss: 7237.4902\n",
            "Epoch 117/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 185.9235 - val_loss: 7524.6055\n",
            "Epoch 118/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 245.3220 - val_loss: 7478.0327\n",
            "Epoch 119/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 204.6602 - val_loss: 7275.5200\n",
            "Epoch 120/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 134.6727 - val_loss: 7301.1152\n",
            "Epoch 121/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 266.9528 - val_loss: 7203.6250\n",
            "Epoch 122/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 266.7859 - val_loss: 7228.4336\n",
            "Epoch 123/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 285.5429 - val_loss: 7504.6611\n",
            "Epoch 124/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 680.9977 - val_loss: 7463.4614\n",
            "Epoch 125/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 573.9294 - val_loss: 7189.2852\n",
            "Epoch 126/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 393.8751 - val_loss: 7626.8652\n",
            "Epoch 127/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 137.9278 - val_loss: 7143.6943\n",
            "Epoch 128/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 152.4038 - val_loss: 7211.8794\n",
            "Epoch 129/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 173.5852 - val_loss: 6807.5820\n",
            "Epoch 130/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 417.4124 - val_loss: 7551.1719\n",
            "Epoch 131/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 537.8105 - val_loss: 7682.5801\n",
            "Epoch 132/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 777.6249 - val_loss: 8977.0410\n",
            "Epoch 133/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3841.1604 - val_loss: 8786.3701\n",
            "Epoch 134/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2579.0950 - val_loss: 8987.6904\n",
            "Epoch 135/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3657.6145 - val_loss: 15665.5400\n",
            "Epoch 136/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5639.5786 - val_loss: 17892.5645\n",
            "Epoch 137/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23136.7539 - val_loss: 74797.4453\n",
            "Epoch 138/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 65548.4219 - val_loss: 19100.2168\n",
            "Epoch 139/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 15938.7988 - val_loss: 25941.8027\n",
            "Epoch 140/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 27178.7988 - val_loss: 81093.1562\n",
            "Epoch 141/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 78413.3125 - val_loss: 105596.8594\n",
            "Epoch 142/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 125388.3672 - val_loss: 130281.3438\n",
            "Epoch 143/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 176733.6875 - val_loss: 513990.4062\n",
            "Epoch 144/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 613132.0000 - val_loss: 798242.5625\n",
            "Epoch 145/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 757193.9375 - val_loss: 227358.5781\n",
            "Epoch 146/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 204961.1094 - val_loss: 75950.7188\n",
            "Epoch 147/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 102098.8125 - val_loss: 71102.9219\n",
            "Epoch 148/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 101423.9766 - val_loss: 75607.0625\n",
            "Epoch 149/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 58397.2305 - val_loss: 57069.3867\n",
            "Epoch 150/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 98320.3047 - val_loss: 30356.9453\n",
            "Epoch 151/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 39768.6484 - val_loss: 24685.5605\n",
            "Epoch 152/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14931.0039 - val_loss: 11548.8271\n",
            "Epoch 153/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1953.7213 - val_loss: 8039.3965\n",
            "Epoch 154/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 830.8907 - val_loss: 7427.0498\n",
            "Epoch 155/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 286.0540 - val_loss: 8148.4883\n",
            "Epoch 156/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 235.3036 - val_loss: 7776.8623\n",
            "Epoch 157/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 254.5660 - val_loss: 7173.1812\n",
            "Epoch 158/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 140.8411 - val_loss: 7213.0786\n",
            "Epoch 159/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 201.3061 - val_loss: 6907.8481\n",
            "Epoch 160/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 162.6238 - val_loss: 7163.0107\n",
            "Epoch 161/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 119.0721 - val_loss: 6909.5864\n",
            "Epoch 162/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 119.3223 - val_loss: 7361.0068\n",
            "Epoch 163/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 256.5403 - val_loss: 7702.4316\n",
            "Epoch 164/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 254.6189 - val_loss: 7516.0874\n",
            "Epoch 165/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 204.5763 - val_loss: 6868.5420\n",
            "Epoch 166/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 214.7170 - val_loss: 7163.6011\n",
            "Epoch 167/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 614.8246 - val_loss: 8227.3525\n",
            "Epoch 168/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 909.8671 - val_loss: 8231.5566\n",
            "Epoch 169/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 397.0685 - val_loss: 7401.0015\n",
            "Epoch 170/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 223.3729 - val_loss: 8004.5513\n",
            "Epoch 171/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 523.9963 - val_loss: 7506.4102\n",
            "Epoch 172/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 308.0470 - val_loss: 7988.0996\n",
            "Epoch 173/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 283.5179 - val_loss: 6907.5327\n",
            "Epoch 174/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 135.9308 - val_loss: 7068.4805\n",
            "Epoch 175/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 145.4278 - val_loss: 7070.7974\n",
            "Epoch 176/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 141.9618 - val_loss: 7673.2539\n",
            "Epoch 177/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 232.4523 - val_loss: 7140.0811\n",
            "Epoch 178/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 325.6910 - val_loss: 6862.4932\n",
            "Epoch 179/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 316.2050 - val_loss: 7626.9702\n",
            "Epoch 180/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 569.6834 - val_loss: 7645.5718\n",
            "Epoch 181/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 913.0782 - val_loss: 7325.1875\n",
            "Epoch 182/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1634.2252 - val_loss: 6868.7524\n",
            "Epoch 183/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1498.2787 - val_loss: 7292.2251\n",
            "Epoch 184/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1132.9854 - val_loss: 8398.9502\n",
            "Epoch 185/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3008.5107 - val_loss: 22498.9316\n",
            "Epoch 186/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 33535.1992 - val_loss: 94034.3906\n",
            "Epoch 187/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 144997.6875 - val_loss: 280229.6250\n",
            "Epoch 188/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 175482.5312 - val_loss: 298878.4062\n",
            "Epoch 189/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 236300.5781 - val_loss: 126542.3516\n",
            "Epoch 190/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 91436.4844 - val_loss: 43509.4609\n",
            "Epoch 191/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 32712.5078 - val_loss: 19149.5703\n",
            "Epoch 192/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 10661.6875 - val_loss: 15540.7715\n",
            "Epoch 193/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4470.4092 - val_loss: 8236.4648\n",
            "Epoch 194/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3762.2668 - val_loss: 13182.7197\n",
            "Epoch 195/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4996.6953 - val_loss: 11477.6348\n",
            "Epoch 196/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 4820.4854 - val_loss: 7943.4287\n",
            "Epoch 197/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2343.2964 - val_loss: 9350.1250\n",
            "Epoch 198/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4250.6582 - val_loss: 16026.0859\n",
            "Epoch 199/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7935.1880 - val_loss: 9753.3555\n",
            "Epoch 200/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 974.9866 - val_loss: 7646.8301\n",
            "Epoch 201/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 459.0737 - val_loss: 7610.4360\n",
            "Epoch 202/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 824.2902 - val_loss: 9030.2578\n",
            "Epoch 203/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1246.0756 - val_loss: 8298.1514\n",
            "Epoch 204/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 686.7122 - val_loss: 8414.3535\n",
            "Epoch 205/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 991.1754 - val_loss: 7457.3765\n",
            "Epoch 206/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1258.0359 - val_loss: 6965.5688\n",
            "Epoch 207/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 557.7704 - val_loss: 7305.2769\n",
            "Epoch 208/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 310.4172 - val_loss: 6934.1938\n",
            "Epoch 209/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1748.1959 - val_loss: 18455.0137\n",
            "Epoch 210/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 14453.1943 - val_loss: 39812.5234\n",
            "Epoch 211/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 84689.8125 - val_loss: 39583.5781\n",
            "Epoch 212/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 73746.1250 - val_loss: 536275.6250\n",
            "Epoch 213/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 584496.0625 - val_loss: 225117.4375\n",
            "Epoch 214/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 181675.8906 - val_loss: 330934.2812\n",
            "Epoch 215/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 553491.4375 - val_loss: 271977.8125\n",
            "Epoch 216/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 159070.2812 - val_loss: 114047.5938\n",
            "Epoch 217/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 98693.6875 - val_loss: 21331.2871\n",
            "Epoch 218/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 16679.9355 - val_loss: 9169.9746\n",
            "Epoch 219/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3078.1860 - val_loss: 10106.9902\n",
            "Epoch 220/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2425.4827 - val_loss: 10092.7930\n",
            "Epoch 221/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1710.5327 - val_loss: 9386.6748\n",
            "Epoch 222/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1197.7745 - val_loss: 8488.6709\n",
            "Epoch 223/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1394.7745 - val_loss: 8008.5439\n",
            "Epoch 224/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 379.0085 - val_loss: 6821.1089\n",
            "Epoch 225/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 207.4741 - val_loss: 7473.2871\n",
            "Epoch 226/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 315.1699 - val_loss: 7832.8511\n",
            "Epoch 227/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 357.1482 - val_loss: 7495.2515\n",
            "Epoch 228/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 350.8577 - val_loss: 7319.2388\n",
            "Epoch 229/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 236.8534 - val_loss: 7206.3594\n",
            "Epoch 230/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 111.5005 - val_loss: 7062.0605\n",
            "Epoch 231/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 247.7078 - val_loss: 7237.4102\n",
            "Epoch 232/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 211.4063 - val_loss: 6930.9126\n",
            "Epoch 233/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 93.5267 - val_loss: 7144.1436\n",
            "Epoch 234/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 90.4899 - val_loss: 6733.2900\n",
            "Epoch 235/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 170.0456 - val_loss: 7815.1812\n",
            "Epoch 236/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 659.3826 - val_loss: 6743.3901\n",
            "Epoch 237/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 338.7861 - val_loss: 7184.2715\n",
            "Epoch 238/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 241.3099 - val_loss: 7042.9639\n",
            "Epoch 239/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 180.3470 - val_loss: 7058.0137\n",
            "Epoch 240/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 64.2627 - val_loss: 7046.2749\n",
            "Epoch 241/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 47.3355 - val_loss: 7307.3789\n",
            "Epoch 242/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 92.0246 - val_loss: 6887.2573\n",
            "Epoch 243/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 100.6177 - val_loss: 6850.6255\n",
            "Epoch 244/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 469.0873 - val_loss: 7216.5068\n",
            "Epoch 245/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 193.2639 - val_loss: 6960.2485\n",
            "Epoch 246/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 237.6847 - val_loss: 7168.6724\n",
            "Epoch 247/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 504.3750 - val_loss: 7148.9526\n",
            "Epoch 248/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 264.1447 - val_loss: 7153.2754\n",
            "Epoch 249/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 125.5230 - val_loss: 6647.6411\n",
            "Epoch 250/250\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 106.1488 - val_loss: 6872.9312\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x798069275000>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi model\n",
        "loss = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Test loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhGurwkHXI55",
        "outputId": "2b7637f8-ab16-448a-aca6-f6949ea74b1f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6166.4233  \n",
            "Test loss: 6872.93115234375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan prediksi vs nilai sebenarnya\n",
        "print(\"Predictions: \", predictions.flatten())\n",
        "print(\"Actual values: \", y_test.values)\n",
        "\n",
        "# Menghitung koefisien model (Optional: jika Anda ingin melihat pengaruh masing-masing fitur)\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "print(\"Model Weights (Koefisien): \", weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unXO_h9WXRuT",
        "outputId": "1a99ac91-e238-4459-cce3-c5ccbb68b1c1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:  [173025.9    41848.14  175722.5   218397.56   49713.355 144525.98\n",
            "  54126.473 163633.81   34653.32   88018.01  194980.66   93958.16\n",
            " 193533.19  199559.52  196013.97  177108.95   22701.088  43402.594\n",
            " 101085.086  99783.97  116440.95  129659.41   36939.81  221122.38\n",
            "  62269.676  86449.15  144850.53   46496.63  133456.16  165257.53\n",
            " 132547.77  201888.47  147012.42  126596.88  205915.11   56041.77\n",
            "  91784.28   62275.973 125064.17  167383.16  184776.16   85699.53\n",
            "  69091.28   27615.115 203814.45  185077.44   95553.28  186337.94\n",
            " 198986.98   24250.244 190284.95  137195.75   66603.59   96112.35\n",
            "  52620.32  212926.75  183263.25   31602.174 189104.22  106976.45\n",
            "  95416.89   93003.09  194097.28  146898.08   83142.56  203449.83\n",
            "  79028.33  216191.25  190660.47  101930.83  208611.19  100952.52\n",
            "  94346.05   61518.426 168761.75   60582.004  87628.64   97992.266\n",
            " 180728.25   48202.25  175836.67  177930.5   137980.55   42652.637\n",
            "  43943.812  50203.004 185718.33  151500.5    93318.5    91234.46\n",
            " 174690.42  170720.83   45763.465 206996.81   46817.02   22513.715\n",
            " 137115.05   37012.414  36329.06  194540.17  149759.16  162365.5\n",
            " 123557.84  101192.39  145294.19  122954.23   50457.77  201328.84\n",
            "  81222.79  164139.16   70783.086 179616.2   101086.78  227220.\n",
            "  70758.95  206857.62  177090.9    39551.53  157988.34  125098.055\n",
            "  66089.47  164748.38   15765.791 180598.66  141445.33   38187.8\n",
            "  18538.875 136747.88   99012.84  154715.5   105000.1   179743.22\n",
            " 191832.56  157061.22  157417.86  193428.25   27444.281 103013.64\n",
            " 206244.8    42381.824 221939.88   24112.277 168420.05  188944.75\n",
            "  78928.66  204732.4    80798.93  181804.12   50493.184 115949.234\n",
            "  35300.56   43870.66   48013.13  157637.16  102345.36  101416.64\n",
            " 134482.44   63945.84  134533.67   27871.156  75658.625  67241.56\n",
            " 161088.1    86452.85  202113.88  181940.62   38470.67   81863.43\n",
            " 216158.69  130804.7    41388.883  79248.39   78650.21   24688.16\n",
            "  36936.4   147670.88   28757.633 212068.97   44711.89  137514.6\n",
            "  92139.22  205216.19   44921.285 208312.16  109920.445 140657.16\n",
            " 121543.42   62920.105 112577.55  102866.91   68163.45   62792.504\n",
            " 109101.234 130459.03  131227.25  162761.88   78322.17   22534.436\n",
            "  19486.24  120935.42 ]\n",
            "Actual values:  [172769  39732 175684 218498  51252 144758  52364 162646  35552  88001\n",
            " 195166  93825 193247 199509 196026 177109  19985  44352 101209 100461\n",
            " 115367 129545  36992 221483  62680  86380 145028  47363 133987 164894\n",
            " 132463 201738 146757 126993 206073  55213  92649  63744 125062 167441\n",
            " 184825  85372  70294  27195 203917 184343  95675 186340 199066  23939\n",
            " 190218 137164  67312  96558  52151 212829 183338  31707 188785 107307\n",
            "  95623  92459 193711 146590  83369 203613  79970 215997 190667 102265\n",
            " 208680 101485  94727  61369 168738  61691  87895  97339 180824  50228\n",
            " 175962 177735 137160  41742  42408  50910 185208 151952  93442  91101\n",
            " 174392 169940  45006 206241  45424  22800 137327  35140  36513 194561\n",
            " 149781 162800 123901 101550 145161 123351  51665 201373  81636 164454\n",
            "  72305 179122 100913 227228  70348 206225 177285  39347 158113 125559\n",
            "  66086 164577  15017 180714 141663  37513  16942 136208  98802 154647\n",
            " 105249 179945 191714 156748 157344 193569  27397 102512 206260  42197\n",
            " 221695  20612 168569 189010  78906 204655  81807 181872  52400 115879\n",
            "  34564  45270  48553 157763 102211 101188 135193  63665 135036  29853\n",
            "  75514  68681 160881  86530 202348 182426  38237  81410 215709 129915\n",
            "  41308  78474  78779  23938  38867 147675  28962 212298  45203 137532\n",
            "  91471 204772  41836 207993 109615 140744 121732  62223 112495 103265\n",
            "  68511  64159 109451 130639 130924 162851  78483  21568  17338 120371]\n",
            "Model Weights (Koefisien):  [[ 1.98107958e-01  1.92496225e-01  2.05393985e-01 ...  1.76037654e-01\n",
            "   1.06416240e-01  1.85140252e-01]\n",
            " [ 8.26702919e-04 -1.35801332e-02 -2.52806544e-02 ... -8.00883956e-03\n",
            "  -7.25439489e-02 -4.42413092e-02]\n",
            " [-3.89003055e-03 -2.90806894e-03  7.02659180e-03 ... -5.55278733e-03\n",
            "  -6.30775616e-02  1.30941928e-03]\n",
            " ...\n",
            " [-3.43102758e-04 -1.19630175e-04 -1.11425025e-02 ... -3.10940947e-03\n",
            "  -1.75011195e-02 -4.87676170e-03]\n",
            " [-1.96018815e-03 -1.59808919e-02 -1.87862664e-02 ...  2.91671860e-03\n",
            "   7.88669568e-03  1.32401045e-02]\n",
            " [-6.72422489e-03 -1.08180912e-02  3.39446735e-04 ...  3.29711428e-03\n",
            "  -5.97120682e-03  6.28039427e-03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = np.array([[120000, 10000, 4000, 3600, 2600, 13000, 2840, 2800]])  # Contoh data baru\n",
        "X_new_scaled = scaler.transform(X_new)\n",
        "prediksi = model.predict(X_new_scaled)\n",
        "print(f\"Prediksi laba bersih: {prediksi[0][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu88Ack3BVl5",
        "outputId": "447a76f6-2af3-4b86-c942-64c4fdc76ded"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "Prediksi laba bersih: 81241.5859375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}